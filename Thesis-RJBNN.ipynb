{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using Gen\n",
    "using Flux\n",
    "using JLD\n",
    "using Random\n",
    "using StatsBase\n",
    "using LinearAlgebra\n",
    "using PyPlot\n",
    "using Distributions\n",
    "\n",
    "include(\"LoadData.jl\")\n",
    "include(\"NUTS.jl\")\n",
    "include(\"proposals.jl\")\n",
    "include(\"utils.jl\")\n",
    "include(\"hmc_mod.jl\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load Boston Housing Data\n",
    "\n",
    "dx, dy, x_train, x_test, y_train, y_test = load_data(10);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Network hyperparameters\n",
    "k_real = 1 #Number of hidden nodes per layer\n",
    "k_vector = [0.0 for i=1:k_real]\n",
    "k_vector[k_real] = 1.0\n",
    "\n",
    "#Layer hyperparameters\n",
    "l_range = 8 #Maximum number of layers in the network\n",
    "l_list = [Int(i) for i in 1:l_range]\n",
    "l_real = 1;\n",
    "\n",
    "#Hyperprior Hyperparameters\n",
    "αᵧ = 1 #Regression Noise Shape\n",
    "βᵧ = 1 #Regression Noise Scale/Rate\n",
    "α₁ = 1 #Input Weights, Biases Shape\n",
    "β₁ = 1 #Input Weights, Biases Scale/Rate\n",
    "α₂ = 1 #Hidden & Output Weights Shape\n",
    "β₂ = k_real; #Hidden & Output Weights Scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Bayesian Neural Network \n",
    "\n",
    "function G(x, trace)\n",
    "    activation = relu\n",
    "    l = trace[:l]\n",
    "    ks = [trace[(:k,i)] for i=1:l]\n",
    "    \n",
    "    for i=1:l\n",
    "        in_dim, out_dim = layer_unpacker(i, l, ks)\n",
    "        W = reshape(trace[(:W,i)], out_dim, in_dim)\n",
    "        b = reshape(trace[(:b,i)], trace[(:k,i)])\n",
    "        nn = Dense(W, b, activation)\n",
    "        x = nn(x)\n",
    "    end\n",
    "    \n",
    "    Wₒ = reshape(trace[(:W,l+1)], 1, ks[l])\n",
    "    bₒ = reshape(trace[(:b,l+1)], 1)\n",
    "    \n",
    "    nn_out = Dense(Wₒ, bₒ)\n",
    "    return nn_out(x)\n",
    "    \n",
    "end;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Probabilistic Model\n",
    "\n",
    "@gen function interpolator(x)\n",
    "    \n",
    "    d = length(x[:,1])\n",
    "    \n",
    "    #Create a blank choicemap\n",
    "    obs = choicemap()::ChoiceMap\n",
    "    \n",
    "    #Draw number of layers\n",
    "    l ~ categorical([1/length(l_list) for i=1:length(l_list)])\n",
    "    l_real = l\n",
    "    obs[:l] = l\n",
    "    \n",
    "    #Create individual weight and bias vectors\n",
    "    #Loop through hidden layers\n",
    "    k = [Int(0) for i=1:l+1]\n",
    "    for i=1:l\n",
    "        k[i] = @trace(categorical(k_vector), (:k,i))\n",
    "        obs[(:k,i)] = k[i]\n",
    "    end\n",
    "    k[l+1] = @trace(categorical([1.0]), (:k,l+1))\n",
    "    obs[(:k,l+1)] = k[l+1]\n",
    "    \n",
    "    #####################################\n",
    "    #New hyperparameter schedule - Mar 8#\n",
    "    #####################################\n",
    "    \n",
    "    τ = [0.0 for i=1:l+1]\n",
    "    τᵦ = [0.0 for i=1:l+1]\n",
    "    σ = [0.0 for i=1:l+1]\n",
    "    σᵦ = [0.0 for i=1:l+1]\n",
    "    \n",
    "    for i=1:l+1\n",
    "        if i==1\n",
    "            τ[i] = @trace(gamma(α₁,β₁), (:τ,i))\n",
    "            τᵦ[i] = @trace(gamma(α₁,β₁), (:τᵦ,i))\n",
    "        else\n",
    "            τ[i] = @trace(gamma(α₂,β₂), (:τ,i))\n",
    "            τᵦ[i] = @trace(gamma(α₁,β₁), (:τᵦ,i))\n",
    "        end\n",
    "        σ[i] = 1/τ[i]\n",
    "        σᵦ[i] = 1/τᵦ[i]\n",
    "    end\n",
    "    \n",
    "    #Noise Variance\n",
    "    τᵧ ~ gamma(αᵧ,βᵧ)\n",
    "    σᵧ = 1/τᵧ\n",
    "    \n",
    "    #Sample weight and bias vectors\n",
    "    W = [zeros(k[i]) for i=1:l+1]\n",
    "    b = [zeros(k[i]) for i=1:l+1]\n",
    "\n",
    "    for i=1:l+1\n",
    "        if i == 1\n",
    "            h = Int(d * k[i])\n",
    "        else\n",
    "            h = Int(k[i-1] * k[i])\n",
    "        end\n",
    "\n",
    "        if i<=l\n",
    "            #Hidden Weights\n",
    "            u = zeros(h)\n",
    "            S = Diagonal([σ[i] for j=1:length(u)])::Diagonal{<:Real}\n",
    "            W[i] = @trace(mvnormal(u,S), (:W,i))\n",
    "            obs[(:W,i)] = W[i]\n",
    "            \n",
    "            #Hidden Biases\n",
    "            ub = zeros(k[i])\n",
    "            Sb = Diagonal([σᵦ[i] for j=1:length(ub)])::Diagonal{<:Real}    \n",
    "            b[i] = @trace(mvnormal(ub,Sb), (:b,i))\n",
    "            obs[(:b,i)] = b[i]\n",
    "        else\n",
    "            #Output Weights\n",
    "            u = zeros(k[l])\n",
    "            S = Diagonal([σ[i] for j=1:length(u)])::Diagonal{<:Real}\n",
    "            W[i] = @trace(mvnormal(u,S), (:W,i))\n",
    "            obs[(:W,i)] = W[i]\n",
    "\n",
    "            #Output Bias\n",
    "            ub = zeros(1)\n",
    "            Sb = Diagonal([σᵦ[i] for j=1:length(ub)])::Diagonal{<:Real}  \n",
    "            b[i] = @trace(mvnormal(ub,Sb), (:b,i))\n",
    "            obs[(:b,i)] = b[i]\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    #Return Network Scores for X\n",
    "    scores = transpose(G(x,obs))[:,1]\n",
    "    \n",
    "    #Regression Likelihood\n",
    "    Sy = Diagonal([σᵧ for i=1:length(x[1,:])])::Diagonal{<:Real}\n",
    "    y = @trace(mvnormal(vec(scores), Sy), (:y))\n",
    "\n",
    "    return scores\n",
    "    \n",
    "end\n",
    "\n",
    "obs_master = choicemap()::ChoiceMap\n",
    "obs_master[:y] = y_train\n",
    "obs = obs_master;\n",
    "\n",
    "best_trace = find_best_trace(x_train,y_train,1000);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Straight NUTS\n",
    "include(\"NUTS.jl\")\n",
    "obs_master = choicemap()::ChoiceMap\n",
    "obs_master[:y] = y_train\n",
    "obs = obs_master;\n",
    "\n",
    "Δ_max = 1000\n",
    "#(trace,) = generate(interpolator, (x_train,), obs)\n",
    "trace = best_trace\n",
    "println(trace[:l])\n",
    "sigma = 1/(trace[:τᵧ]) #Best so far: 3.0483\n",
    "println(\"$sigma\")\n",
    "\n",
    "#Trace 1\n",
    "param_selection = select()\n",
    "\n",
    "for i=1:trace[:l]+1 #Number of Layers\n",
    "    push!(param_selection, (:W,i))\n",
    "    push!(param_selection, (:b,i))\n",
    "end\n",
    "\n",
    "m=100\n",
    "\n",
    "traces = NUTS(trace, param_selection, 0.65, m+1, 2, true); #m+1, m=100, 0.65\n",
    "\n",
    "plot([get_score(trace) for trace in traces])\n",
    "plt.title(\"NUTS Score: Boston Housing Price\")\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"Log Posterior\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Layer-at-a-time NUTS\n",
    "Δ_max = 1000\n",
    "trace = best_trace\n",
    "\n",
    "function load_layer(l)\n",
    "    layer_selection = select()\n",
    "    push!(layer_selection, (:W,l))\n",
    "    push!(layer_selection, (:b,l))\n",
    "    return layer_selection\n",
    "end\n",
    "\n",
    "@gen function gibbs_hyperparameters(trace)\n",
    "    obs_new = choicemap()::ChoiceMap\n",
    "    args = get_args(trace)\n",
    "    argdiffs = map((_) -> NoChange(), args)\n",
    "    \n",
    "    for i=1:trace[:l] + 1\n",
    "        #Biases\n",
    "        bias = trace[(:b,i)]\n",
    "        \n",
    "        n = length(bias)\n",
    "        α = α₁ + (n/2)\n",
    "        \n",
    "        Σ = sum(bias.^2)/2 \n",
    "        β = 1/(1/β₁ + Σ)\n",
    "        \n",
    "        τᵦ ~ gamma(α,β)\n",
    "        \n",
    "        #Weights\n",
    "        i == 1 ? α₀ = α₁ : α₀ = α₂\n",
    "        i == 1 ? β₀ = β₁ : β₀ = β₂\n",
    "        \n",
    "        weight = trace[(:W,i)]\n",
    "        \n",
    "        n = length(weight)\n",
    "        α = α₀ + (n/2)\n",
    "        \n",
    "        Σ = sum(weight.^2)/2\n",
    "        β = 1/(1/β₀ + Σ)\n",
    "        \n",
    "        τ ~ gamma(α,β)\n",
    "        \n",
    "        obs_new[(:τ,i)] = τ\n",
    "        obs_new[(:τᵦ,i)] = τᵦ\n",
    "    end\n",
    "    \n",
    "    (new_trace,_,_,_) = update(trace, args, argdiffs, obs_new)\n",
    "    \n",
    "    return new_trace\n",
    "end\n",
    "\n",
    "@gen function gibbs_noise(trace)\n",
    "    obs_new = choicemap()::ChoiceMap\n",
    "    args = get_args(trace)\n",
    "    argdiffs = map((_) -> NoChange(), args)\n",
    "    \n",
    "    n = length(trace[:y])\n",
    "    α = αᵧ + (n/2)\n",
    "    \n",
    "    x = get_args(trace)[1]\n",
    "    y_pred = transpose(G(x,trace))[:,1]\n",
    "    y_real = trace[:y]\n",
    "    Σᵧ = sum((y_pred .- y_real).^2)/2\n",
    "    β = 1/(1/βᵧ + Σᵧ)\n",
    "    \n",
    "    τ ~ gamma(α,β)\n",
    "    obs_new[:τᵧ] = τ\n",
    "    \n",
    "    (new_trace,_,_,_) = update(trace, args, argdiffs, obs_new)\n",
    "    \n",
    "    return new_trace\n",
    "end\n",
    "\n",
    "\n",
    "m=1\n",
    "iters=30\n",
    "\n",
    "#(best_trace,) = generate(interpolator, (x_train,), obs)\n",
    "best_layer = best_trace[:l]\n",
    "println(\"Starting layers: $best_layer\")\n",
    "traces = []\n",
    "push!(traces,best_trace)\n",
    "\n",
    "for i=1:iters\n",
    "    println(\"Iteration $i\")\n",
    "    trace_star = traces[i]\n",
    "    trace_star = gibbs_hyperparameters(trace_star)\n",
    "    trace_star = gibbs_noise(trace_star)\n",
    "    for j=1:trace_star[:l]+1\n",
    "        v = trace_star[:l]+2 - j\n",
    "        layer_selection = load_layer(v)\n",
    "        trace_star = NUTS(trace_star, layer_selection, 0.15, m, m, true)[m+1]\n",
    "    end\n",
    "    push!(traces,trace_star)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-------------\n",
    "#Run Inference\n",
    "#-------------\n",
    "\n",
    "include(\"NUTS.jl\")\n",
    "Δ_max = 1\n",
    "acc_prob = 0.35\n",
    "#Random.seed!(1)\n",
    "\n",
    "scores = []\n",
    "traces = []\n",
    "ks = []\n",
    "across_acceptance = []\n",
    "within_acceptance = []\n",
    "\n",
    "#Inference Hyperparameters\n",
    "iters = 1\n",
    "m = 3\n",
    "\n",
    "function load_layer(l)\n",
    "    layer_selection = select()\n",
    "    push!(layer_selection, (:W,l))\n",
    "    push!(layer_selection, (:b,l))\n",
    "    return layer_selection\n",
    "end\n",
    "\n",
    "@gen function gibbs_hyperparameters(trace)\n",
    "    obs_new = choicemap()::ChoiceMap\n",
    "    args = get_args(trace)\n",
    "    argdiffs = map((_) -> NoChange(), args)\n",
    "    \n",
    "    for i=1:trace[:l] + 1\n",
    "        #Biases\n",
    "        bias = trace[(:b,i)]\n",
    "        \n",
    "        n = length(bias)\n",
    "        α = α₁ + (n/2)\n",
    "        \n",
    "        Σ = sum(bias.^2)/2 \n",
    "        β = 1/(1/β₁ + Σ)\n",
    "        \n",
    "        τᵦ ~ gamma(α,β)\n",
    "        \n",
    "        #Weights\n",
    "        i == 1 ? α₀ = α₁ : α₀ = α₂\n",
    "        i == 1 ? β₀ = β₁ : β₀ = β₂\n",
    "        \n",
    "        weight = trace[(:W,i)]\n",
    "        \n",
    "        n = length(weight)\n",
    "        α = α₀ + (n/2)\n",
    "        \n",
    "        Σ = sum(weight.^2)/2\n",
    "        β = 1/(1/β₀ + Σ)\n",
    "        \n",
    "        τ ~ gamma(α,β)\n",
    "        \n",
    "        obs_new[(:τ,i)] = τ\n",
    "        obs_new[(:τᵦ,i)] = τᵦ\n",
    "    end\n",
    "    \n",
    "    (new_trace,_,_,_) = update(trace, args, argdiffs, obs_new)\n",
    "    \n",
    "    return new_trace\n",
    "end\n",
    "\n",
    "@gen function gibbs_noise(trace)\n",
    "    obs_new = choicemap()::ChoiceMap\n",
    "    args = get_args(trace)\n",
    "    argdiffs = map((_) -> NoChange(), args)\n",
    "    \n",
    "    n = length(trace[:y])\n",
    "    α = αᵧ + (n/2)\n",
    "    \n",
    "    x = get_args(trace)[1]\n",
    "    y_pred = transpose(G(x,trace))[:,1]\n",
    "    y_real = trace[:y]\n",
    "    Σᵧ = sum((y_pred .- y_real).^2)/2\n",
    "    β = 1/(1/βᵧ + Σᵧ)\n",
    "    \n",
    "    τ ~ gamma(α,β)\n",
    "    obs_new[:τᵧ] = τ\n",
    "    \n",
    "    (new_trace,_,_,_) = update(trace, args, argdiffs, obs_new)\n",
    "    \n",
    "    return new_trace\n",
    "end\n",
    "\n",
    "function nuts_parameters(trace)\n",
    "    \n",
    "    l = trace[:l]\n",
    "    param_selection = select()\n",
    "    for i=1:l+1 #Number of Layers\n",
    "        push!(param_selection, (:W,i))\n",
    "        push!(param_selection, (:b,i))\n",
    "    end\n",
    "    \n",
    "    prev_score = get_score(trace)\n",
    "    \n",
    "    acc = 0\n",
    "    for i=1:iters\n",
    "        new_trace = NUTS(trace, param_selection, acc_prob, m, m, false)[m+1]\n",
    "        new_score = get_score(new_trace)\n",
    "        if prev_score != new_score\n",
    "            return (new_trace, 1)\n",
    "        else\n",
    "            return (trace, 0)\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    return (trace, acc)\n",
    "end\n",
    "\n",
    "function layer_nuts(trace,mode=\"draw\")\n",
    "    prev_score = get_score(trace)\n",
    "    new_trace = trace\n",
    "    if mode == \"draw\"\n",
    "        mode = bernoulli(0.5) ? \"forward\" : \"backward\"\n",
    "    end\n",
    "    \n",
    "    #Backward Pass\n",
    "    if mode == \"backward\"\n",
    "        for j=1:new_trace[:l]+1\n",
    "            v = new_trace[:l]+2 - j\n",
    "            println(\"Current Layer: $v\")\n",
    "            layer_selection = load_layer(v)\n",
    "            new_trace = NUTS(new_trace, layer_selection, acc_prob, 4*m, 4*m, false)[m+1]\n",
    "        end\n",
    "        \n",
    "    #Forward Pass\n",
    "    else\n",
    "        for j=1:new_trace[:l]+1\n",
    "            v = j\n",
    "            println(\"Current Layer: $v\")\n",
    "            layer_selection = load_layer(v)\n",
    "            new_trace = NUTS(new_trace, layer_selection, acc_prob, 4*m, 4*m, false)[m+1]\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    new_score = get_score(new_trace)\n",
    "    if prev_score != new_score\n",
    "        return (new_trace, 1)\n",
    "    else\n",
    "        return (trace, 0)\n",
    "    end\n",
    "    \n",
    "end\n",
    "\n",
    "function layer_parameter(trace)\n",
    "    obs = obs_master\n",
    "    for i=1:trace[:l]+1\n",
    "        obs[(:τ,i)] = trace[(:τ,i)]\n",
    "        obs[(:τᵦ,i)] = trace[(:τᵦ,i)]\n",
    "    end\n",
    "    obs[:τᵧ] = trace[:τᵧ]\n",
    "    \n",
    "    init_trace = trace\n",
    "    \n",
    "    #################################################RJNUTS#################################################\n",
    "    #NUTS Step 1\n",
    "    trace_tilde = trace\n",
    "    for i=1:iters\n",
    "        (trace_tilde,) = nuts_parameters(trace_tilde)\n",
    "        #(trace_tilde,) = layer_nuts(trace_tilde,\"forward\")\n",
    "    end\n",
    "    \n",
    "    #Reversible Jump Step\n",
    "    (trace_prime, q_weight) = layer_change(trace_tilde)\n",
    "    \n",
    "    #NUTS Step 2\n",
    "    trace_star = trace_prime\n",
    "    for i=1:iters\n",
    "        (trace_star,) = nuts_parameters(trace_star)\n",
    "        #(trace_star,) = layer_nuts(trace_star,\"backward\")\n",
    "    end\n",
    "    #################################################RJNUTS#################################################\n",
    "        \n",
    "    model_score = -get_score(init_trace) + get_score(trace_star)\n",
    "    across_score = model_score + q_weight\n",
    "    #println(across_score)\n",
    "    #println(model_score)\n",
    "\n",
    "    if rand() < exp(across_score)\n",
    "        println(\"********** Accepted: $(trace_star[:l]) **********\")\n",
    "        return (trace_star, 1)\n",
    "    else\n",
    "        return (init_trace, 0)\n",
    "    end\n",
    "end\n",
    "\n",
    "obs_master = choicemap()::ChoiceMap\n",
    "obs_master[:y] = y_train\n",
    "obs = obs_master;\n",
    "\n",
    "(trace,) = generate(interpolator, (x_train,), obs)\n",
    "trace = best_trace\n",
    "\n",
    "for i=1:20\n",
    "    (trace, accepted) = layer_parameter(trace)\n",
    "    push!(across_acceptance, accepted)\n",
    "    trace  = gibbs_hyperparameters(trace)\n",
    "    trace  = gibbs_noise(trace)\n",
    "    #(trace, accepted)  = layer_nuts(trace)\n",
    "    (trace, accepted) = nuts_parameters(trace)\n",
    "    push!(within_acceptance, accepted)\n",
    "    push!(scores,get_score(trace))\n",
    "    push!(traces, trace)\n",
    "    println(\"$i : $(get_score(trace))\")\n",
    "    flush(stdout)\n",
    "    if i%10 == 0\n",
    "        a_acc = 100*(sum(across_acceptance)/length(across_acceptance))\n",
    "        w_acc = 100*(sum(within_acceptance)/length(within_acceptance))\n",
    "        println(\"Epoch $i A Acceptance Probability: $a_acc %\")\n",
    "        println(\"Epoch $i W Acceptance Probability: $w_acc %\")\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-----------------------------------\n",
    "#Plot Log Posterior Scores by Trace\n",
    "#-----------------------------------\n",
    "\n",
    "plot([get_score(trace) for trace in traces])\n",
    "plt.title(\"NUTS Score: Boston Housing Price\")\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"Log Posterior\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#----------------------------------------\n",
    "#Plot Regression Noise Variance by Trace\n",
    "#----------------------------------------\n",
    "\n",
    "plot([1/trace[:τᵧ] for trace in traces])\n",
    "plt.title(\"NUTS Noise Variance: Boston Housing Price\")\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"Log Posterior\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#--------------------------------------------\n",
    "#Plot Parameter Precision by Trace: Function\n",
    "#--------------------------------------------\n",
    "\n",
    "function plot_precision(layer)\n",
    "    weights = []\n",
    "    biases = []\n",
    "    for i=1:length(traces)\n",
    "        trace = traces[i]\n",
    "        if layer > trace[:l]+1\n",
    "            push!(weights,NaN)\n",
    "            push!(biases,NaN)\n",
    "        else\n",
    "            push!(weights,trace[(:τ,layer)])\n",
    "            push!(biases,trace[(:τᵦ,layer)])\n",
    "        end\n",
    "    end\n",
    "    plot(weights,label=(\"Layer $layer Weights Precision\"))\n",
    "    plot(biases,label=\"Layer $layer Bias Precision\")\n",
    "    plt.title(\"NUTS Parameter Precision: Boston Housing Price\")\n",
    "    plt.xlabel(\"Iteration\")\n",
    "    plt.ylabel(\"Precision\")\n",
    "    plt.legend();\n",
    "end\n",
    "\n",
    "plot_precision(1)\n",
    "plot_precision(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-------------------------------\n",
    "#Plot RMSE Train Scores by Trace\n",
    "#-------------------------------\n",
    "\n",
    "mses_train = []\n",
    "for i=1:length(traces)\n",
    "    trace = traces[i]\n",
    "    pred_y = transpose(G(x_train,trace))[:,1]\n",
    "    mse = mse_scaled(pred_y,y_train)\n",
    "\n",
    "    push!(mses_train,mse)\n",
    "end\n",
    "\n",
    "mses_test = []\n",
    "for i=1:length(traces)\n",
    "    trace = traces[i]\n",
    "    pred_y = transpose(G(x_test,trace))[:,1]\n",
    "    mse = mse_scaled(pred_y,y_test)\n",
    "\n",
    "    push!(mses_test,mse)\n",
    "end\n",
    "\n",
    "plot(mses_train,label=\"Train\")\n",
    "plot(mses_test, label=\"Test\")\n",
    "plt.title(\"NUTS RMSE: Boston Housing Price\")\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"RMSE\");\n",
    "plt.legend()\n",
    "\n",
    "best_trace = traces[1]\n",
    "for i=1:length(traces)\n",
    "    if get_score(traces[i]) > get_score(best_trace)\n",
    "        best_trace = traces[i]\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#----------------\n",
    "#Marginalize RMSE\n",
    "#----------------\n",
    "j = 5\n",
    "y_marginal = zeros(length(y_train))\n",
    "\n",
    "for i=j:length(traces)\n",
    "    trace = traces[i]\n",
    "    pred_y = transpose(G(x_train,trace))[:,1]\n",
    "    y_marginal += (pred_y/(length(traces)-j))\n",
    "end\n",
    "\n",
    "#display(y_marginal[1:5])\n",
    "#display(y[1:5])\n",
    "\n",
    "mse = mse_scaled(y_marginal, y_train)\n",
    "println(\"Training Set Marginal RMSE: $mse\")\n",
    "\n",
    "j = 5\n",
    "y_marginal = zeros(length(y_test))\n",
    "\n",
    "for i=j:length(traces)\n",
    "    trace = traces[i]\n",
    "    pred_y = transpose(G(x_test,trace))[:,1]\n",
    "    y_marginal += (pred_y/(length(traces)-j))\n",
    "end\n",
    "\n",
    "#display(y_marginal[1:5])\n",
    "#display(y[1:5])\n",
    "\n",
    "mse = mse_scaled(y_marginal, y_test)\n",
    "println(\"Test Set Marginal RMSE: $mse\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#HMC Attempt\n",
    "@gen function gibbs_hyperparameters(trace)\n",
    "    obs_new = choicemap()::ChoiceMap\n",
    "    args = get_args(trace)\n",
    "    argdiffs = map((_) -> NoChange(), args)\n",
    "    \n",
    "    for i=1:trace[:l] + 1\n",
    "        #Biases\n",
    "        bias = trace[(:b,i)]\n",
    "        \n",
    "        n = length(bias)\n",
    "        α = α₁ + (n/2)\n",
    "        \n",
    "        Σ = sum(bias.^2)/2 \n",
    "        β = 1/(1/β₁ + Σ)\n",
    "        \n",
    "        τᵦ ~ gamma(α,β)\n",
    "        \n",
    "        #Weights\n",
    "        i == 1 ? α₀ = α₁ : α₀ = α₂\n",
    "        i == 1 ? β₀ = β₁ : β₀ = β₂\n",
    "        \n",
    "        weight = trace[(:W,i)]\n",
    "        \n",
    "        n = length(weight)\n",
    "        α = α₀ + (n/2)\n",
    "        \n",
    "        Σ = sum(weight.^2)/2\n",
    "        β = 1/(1/β₀ + Σ)\n",
    "        \n",
    "        τ ~ gamma(α,β)\n",
    "        \n",
    "        obs_new[(:τ,i)] = τ\n",
    "        obs_new[(:τᵦ,i)] = τᵦ\n",
    "    end\n",
    "    \n",
    "    (new_trace,_,_,_) = update(trace, args, argdiffs, obs_new)\n",
    "    \n",
    "    return new_trace\n",
    "end\n",
    "\n",
    "@gen function gibbs_noise(trace)\n",
    "    obs_new = choicemap()::ChoiceMap\n",
    "    args = get_args(trace)\n",
    "    argdiffs = map((_) -> NoChange(), args)\n",
    "    \n",
    "    n = length(trace[:y])\n",
    "    α = αᵧ + (n/2)\n",
    "    \n",
    "    x = get_args(trace)[1]\n",
    "    y_pred = transpose(G(x,trace))[:,1]\n",
    "    y_real = trace[:y]\n",
    "    Σᵧ = sum((y_pred .- y_real).^2)/2\n",
    "    β = 1/(1/βᵧ + Σᵧ)\n",
    "    \n",
    "    τ ~ gamma(α,β)\n",
    "    obs_new[:τᵧ] = τ\n",
    "    \n",
    "    (new_trace,_,_,_) = update(trace, args, argdiffs, obs_new)\n",
    "    \n",
    "    return new_trace\n",
    "end\n",
    "\n",
    "l = 1000\n",
    "ϵ = 0.00079\n",
    "\n",
    "function within_move(trace)\n",
    "    l = trace[:l]\n",
    "    param_selection = select()\n",
    "    \n",
    "    for i=1:l+1 #Number of Layers\n",
    "        push!(param_selection, (:W,i))\n",
    "        push!(param_selection, (:b,i))\n",
    "    end\n",
    "    \n",
    "    score1 = get_score(trace)\n",
    "    (new_trace, hmc_score) = hmc_mod(trace, param_selection, L=l, eps=ϵ, check=false, observations=obs)\n",
    "    \n",
    "    score2 = get_score(new_trace)\n",
    "    #println(\"new trace score: $score2\")\n",
    "    #println(\"old trace score: $score1\")\n",
    "    \n",
    "    hmc_score = score2 - score1\n",
    "\n",
    "    if rand(Uniform(0,1)) < exp(hmc_score)\n",
    "        trace = new_trace\n",
    "        accepted = 1.0\n",
    "        #println(\"Accepted\")\n",
    "    else\n",
    "        trace = trace\n",
    "        accepted = 0.0\n",
    "        #println(\"Not Accepted\")\n",
    "    end\n",
    "    push!(within_acceptance, accepted)\n",
    "\n",
    "    return trace\n",
    "end\n",
    "\n",
    "traces = []\n",
    "scores = []\n",
    "across_acceptance = []\n",
    "within_acceptance = []\n",
    "trace = best_trace\n",
    "\n",
    "iters = 1000\n",
    "\n",
    "for i=1:iters\n",
    "    #(trace, accepted) = layer_parameter(trace)\n",
    "    #push!(across_acceptance, accepted)\n",
    "    trace  = gibbs_hyperparameters(trace)\n",
    "    trace  = gibbs_noise(trace)\n",
    "    trace  = within_move(trace)\n",
    "    push!(scores,get_score(trace))\n",
    "    push!(traces, trace)\n",
    "    #println(\"Iter $i : $(get_score(trace))\")\n",
    "    if i%50 == 0\n",
    "        #a_acc = 100*(sum(across_acceptance)/length(across_acceptance))\n",
    "        w_acc = 100*(sum(within_acceptance)/length(within_acceptance))\n",
    "        #println(\"Chain $chain Epoch $i A Acceptance Probability: $a_acc %\")\n",
    "        println(\"Epoch $i W Acceptance Probability: $w_acc %\")\n",
    "    end\n",
    "    flush(stdout)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trace = traces[1000];"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-------------------------------\n",
    "#Plot RMSE Train Scores by Trace\n",
    "#-------------------------------\n",
    "\n",
    "mses_train = []\n",
    "for i=1:length(traces)\n",
    "    trace = traces[i]\n",
    "    pred_y = transpose(G(x_train,trace))[:,1]\n",
    "    mse = mse_scaled(pred_y,y_train)\n",
    "\n",
    "    push!(mses_train,mse)\n",
    "end\n",
    "\n",
    "mses_test = []\n",
    "for i=1:length(traces)\n",
    "    trace = traces[i]\n",
    "    pred_y = transpose(G(x_test,trace))[:,1]\n",
    "    mse = mse_scaled(pred_y,y_test)\n",
    "\n",
    "    push!(mses_test,mse)\n",
    "end\n",
    "\n",
    "plot(mses_train,label=\"Train\")\n",
    "plot(mses_test, label=\"Test\")\n",
    "plt.title(\"NUTS RMSE: Boston Housing Price\")\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"RMSE\");\n",
    "plt.legend()\n",
    "\n",
    "best_trace = traces[1]\n",
    "for i=1:length(traces)\n",
    "    if get_score(traces[i]) > get_score(best_trace)\n",
    "        best_trace = traces[i]\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "taus = [1/traces[i][:τᵧ] for i=1:length(traces)]\n",
    "plot(taus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#----------------\n",
    "#Marginalize RMSE\n",
    "#----------------\n",
    "j = 1\n",
    "y_marginal = zeros(length(y_train))\n",
    "\n",
    "for i=j:length(traces)\n",
    "    trace = traces[i]\n",
    "    pred_y = transpose(G(x_train,trace))[:,1]\n",
    "    y_marginal += (pred_y/(length(traces)-j))\n",
    "end\n",
    "\n",
    "#display(y_marginal[1:5])\n",
    "#display(y[1:5])\n",
    "\n",
    "mse = mse_scaled(y_marginal, y_train)\n",
    "println(\"Training Set Marginal RMSE: $mse\")\n",
    "\n",
    "j = 1\n",
    "y_marginal = zeros(length(y_test))\n",
    "\n",
    "for i=j:length(traces)\n",
    "    trace = traces[i]\n",
    "    pred_y = transpose(G(x_test,trace))[:,1]\n",
    "    y_marginal += (pred_y/(length(traces)-j))\n",
    "end\n",
    "\n",
    "#display(y_marginal[1:5])\n",
    "#display(y[1:5])\n",
    "\n",
    "mse = mse_scaled(y_marginal, y_test)\n",
    "println(\"Test Set Marginal RMSE: $mse\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.4.2",
   "language": "julia",
   "name": "julia-1.4"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.4.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
